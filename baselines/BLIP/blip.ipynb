{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 3.02MB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 45.1kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 916kB/s]\n",
      "100%|██████████| 1.78G/1.78G [00:52<00:00, 36.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/samuelyu/.cache/huggingface/modules/datasets_modules/datasets/facebook--winoground/ce486f3e39fab90997d6f3c58c4b0103eb9c37011049ef775a465f0ab2e78d7d (last modified on Sat Oct  8 23:11:20 2022) since it couldn't be found locally at facebook/winoground., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset winoground (/home/samuelyu/.cache/huggingface/datasets/facebook___winoground/default/0.0.0/ce486f3e39fab90997d6f3c58c4b0103eb9c37011049ef775a465f0ab2e78d7d)\n",
      "100%|██████████| 1/1 [00:00<00:00, 948.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from blip_itm import blip_itm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# adapted from https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def preprocess(image, image_size, device):\n",
    "    # just take the preprocessing method from their demo for now\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "def score(output):\n",
    "    return torch.nn.functional.softmax(output, dim=1)[:, 1].item()\n",
    "\n",
    "image_size = 384 # default for BLIP\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
    "\n",
    "model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model.eval()\n",
    "model = model.to(device=device)\n",
    "\n",
    "auth_token = \"\" # use your own\n",
    "winoground = load_dataset(\"facebook/winoground\", use_auth_token=auth_token)[\"test\"]\n",
    "\n",
    "f = open(\"BLIP_scores.txt\", \"w\")\n",
    "f.write(\"ID\\xa0tag\\xa0secondary_tag\\xa0num_main_preds\\xa0collapsed_tag\\xa0C0I0\\xa0C0I1\\xa0C1I0\\xa0C1I1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for example in winoground:\n",
    "  i0 = preprocess(example[\"image_0\"].convert(\"RGB\"), image_size, device)\n",
    "  i1 = preprocess(example[\"image_1\"].convert(\"RGB\"), image_size, device)\n",
    "  c0 = example[\"caption_0\"]\n",
    "  c1 = example[\"caption_1\"]\n",
    "  output_c0_i0 = model(i0, c0, match_head=\"itm\")\n",
    "  output_c1_i0 = model(i0, c1, match_head=\"itm\")\n",
    "  output_c0_i1 = model(i1, c0, match_head=\"itm\")\n",
    "  output_c1_i1 = model(i1, c1, match_head=\"itm\")\n",
    "  print(output_c0_i0.shape)\n",
    "  break\n",
    "  score_c0_i0 = score(output_c0_i0)\n",
    "  score_c1_i0 = score(output_c1_i0)\n",
    "  score_c0_i1 = score(output_c0_i1)\n",
    "  score_c1_i1 = score(output_c1_i1)\n",
    "  row = [str(example[\"id\"]), example[\"tag\"], example[\"secondary_tag\"], str(example[\"num_main_preds\"]),\n",
    "         example[\"collapsed_tag\"], str(score_c0_i0), str(score_c0_i1), str(score_c1_i0),\n",
    "         str(score_c1_i1)]\n",
    "  f.write(\"\\xa0\".join(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('winoground')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c77539f7a5afcb44bb1dadb52a34ceea8e3af56634fd66545ab473609bb8c1d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
